# [ ] bonus!
# `Update: April 6, 2024`
### `N.B.` references are available in the end of this instruction file!

## `TODO:` 
- Now, at this stage you should have already generated the geocodes (goehash, H3, S2) for point data (csv files) and the corresponding covering for each geocode (applied to `simplified` geojson files), the next task that follows is the following:
- Develop an adapted version of the most famouse Density-based clustering algorithm DBSCAN!. This version of DBSCAN will operate on data sampled using geocodes (H3, S2, geohash) after applying the line simplification, so the number of vertices is reduced, then compare the DBSCAN that is based on the sample with the `plain` DBSCAN that is based on the `population` data in terms of `running time` and `accuracy`, see below for accuracy! 
1. [ ] run the example clustering code (DBSCAN), attached in the new folder "DBSCAN_Clustering" inside 'starting_code' folder
2. [ ] read more about how DBSCAN works in scikit-learn
    > [DBSCAN scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html)
3. [ ] you need to use ```Silhouette Coefficient ```for evaluation
    > ***"If the ground truth labels are not known, evaluation can only be performed using the model results itself. In that case, the Silhouette Coefficient comes in handy."***
    an example is here:
    [Demo of DBSCAN clustering algorithm](https://scikit-learn.org/stable/auto_examples/cluster/plot_dbscan.html#sphx-glr-auto-examples-cluster-plot-dbscan-py)
- Then you need to `adapt` the stock version of the DBSCAN so that it operates a bit differently, specifically you need to do the following:
    - plain scikit-learn DBSCAN distance calculation relies on using  ```haversine``` as a distance metric to calculate haversine distances (i.e., geometrical distances) between coordinates (longitudes/latitudes pairs)
  > the attention that should be given in this case is that you did not capture any statistics regarding the distribution of the pm25 values (our target variable), you could for example capture the histograms of those values.
  Read more about possible values of ```pm2.5``` in this reference [PM2.5 particles in the air](https://www.epa.vic.gov.au/for-community/environmental-information/air-quality/pm25-particles-in-the-air). This means that you need to create a histogram showing the density of each bracket, your binning strategy should rely on the community definition of ranges of values. For example, binning example is the follwoing: Less than 25, 25–50, 50–100, 100–300, More than 300. 
    - Also, draw histograms showing the same binning and density of pm2.5 values in each neighborhood in your data. By the way, how many neighborhoods you have in your data?!
  > this is important as it will inform us about the fact whether nearby locations are having similar pm25 values. Why do we need to do this, because it is only in that case we consider those as a cluster, since they are geographiclly nearby, and also having simialr feature values (pm25 in this case). So what you need to do next is the following:
  - .. ```Extract and normalize several features```, similar to what has been done in the following tutorial, read specifically [Extending DBSCAN beyond just spatial coordinates](https://musa-550-fall-2020.github.io/slides/lecture-11A.html), thereafter ```Run DBSCAN to extract high-density clusters``` passing as an argument to the DBSCAN the new ```scaled features```, probably something like ```cores, labels = dbscan(features, eps=eps, min_samples=min_samples)``` . notice passing features (including pm25, longitude, latitude) instead of simply the coordinates.
  - having done this novel distance calculation (based on geometrical distance and pm25 values distance), calculate again the ```silhouette_score```, and check wether you obtain a higher accuracy (higher silhouette_score values) or not!

  > **N.B.** we are able to do this because of the definition of ```metric``` in DBSCAN which says **metric: The metric to use when calculating distance between instances in** a ```feature array``` so, it  a distance between several features is possible, given a ```feature array```, so put your scaled features in a feature array.
- `THEREAFTER`! You need to create a novel `adapted` version of DBSCAN that is based on the following: 
    - first, the stock version of DBSCAN depends on two main parameters (`epsilon`==> `The maximum distance between two samples for one to be considered as in the neighborhood of the other` and `min_samples` ==> `The number of samples (or total weight) in a neighborhood for a point to be considered as a core point.`). perhaps most importantly is that DBSCAN is an iterative algorithm that needs to fine `core` points (i.e., `cluster centers`) and calculate `distances` (could be `haversine`, `manhattan`, `euclidean`, etc., see the sklearn documentation for further details!, [sklearn.metrics.pairwise_distances](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html)). `however!`, those calculations are computationally expensive when we have big geospatial data (it is a geometric calculation, and geometry is expensive!). So, you need to `minimize` those calculations. But how?!!!
        - If you think about the geocodes that you have generated (being that `S2`, `H3` or `geohash`) where each geocode contains within its premises a set of points (i.e., set long/lat pairs). Think of those geocodes as Minimum Bounding Rectangles (MBR), or Minimum Bounding Boxes (MBB). Having said that, we have already built successfully aa pre-filtering stage! That is to say, those geocodes are prefilters. Then theoritically speaking, points belonging to same geocode (i.e., are within the MBR represented by the geocode) belong to the same cluster even without applying DBSCAN! that is a cheap quick-and-approximate filter!. So, referring back to the parameters, `min_samples`, we simply count the number of points in each geocode (being that `S2`, `H3` or `geohash`), if that count is greater than `min_samples`, we consider this geocode as a cluster (term this `geocode cluster` hereafter for clarity) without having to calculate any distances! (cheap filter), then we pass only the points (call those as `outliers` for reference hereafter) that belong to geocodes that are having points less than the threshold `min_samples` to the plain DBSCAN to find the remaining clusters by appling the expensive distance calculations in DBSCAN. However, we should do this with caveat! as some of those points are geographically close in distance to some of the `geocode cluster` clusters that resulted in the filtering step!, what does this mean. This means that before generating new clusters based on some of those `outlier` points, we need to check whether their distances to some `geocode cluster` cores are less than the threshold `epsilon`, if so the case then they belong to one or more `geocode cluster` clusters, otherwise we pass them to the plain version of the DBSCAN. This is just a propsed algorithm and you are expected to develop it further as per your comprehension of the DBSCAN and its implementation in scikit-learn.
    - You then need to compare this algorithm with the stock version of DBSCAN as implemented in the scikit-learn. You need to capture two metrics: running time of each algorithm and the accuracy metric (for example, use `silhouette_score` from scikit-learn) [**bonus** if you could apply [geosilhouettes](https://pysal.org/esda/notebooks/geosilhouettes.html)]
see `appendix-A` for further related instructions! 